\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath} % For mathematical symbols and environments
\usepackage{biblatex} %Imports biblatex package
\usepackage{mathtools}
\addbibresource{dl-ref.bib} %Import the bibliography file

\title{Deep Learning}
\author{Mairi Hallman}
\date{May 2024}

\begin{document}

\maketitle

\section{Introduction}

\section{A Brief Overview of Tensors}

You are likely familiar with scalars, vectors, and matrices. These can be thought of as analagous data structures in zero, one, and two-dimensions, respectively. When generalizing to \(N\) dimensions, we refer to these collectively as tensors. A scalar is a zero-order tensor, a vector is a first-order tensor, and a matrix is a second-order tensor. A third-order tensor can be visualized as a stack of matrices. A fourth-order tensor would then be a vector of third order tensors. A fifth-order tensor is a matrix of third-order tensors... and so on.

% notation

% fiber and slices

\subsection{Tensor Products}

Tensor additon and subtraction are self-explanatory if matrix addition and subtraction are understood. The same cannot be said for tensor products. Below is an overview of tensor products necessary for the decompositions that will be presented in the next section.

\subsubsection{Outer Product \(\circ\)}

A tensor \(T^{(N)}\) can be expressed as a product of \(N\) vectors. This is called the outer product (denoted \(\circ\)).

\begin{equation}
    T^{(N)} = u_1 \circ u_2 \circ \dots \circ u_N
\end{equation}

% make less hand-wavey

\subsubsection{Kronecker Product \(\otimes\)}

The Konecker product of two matrices \(A\) and B, their Kronecker product is a matrix of the products of each element in \(A\) and the entire matrix \(B\).

\begin{equation}
    A \otimes B = \begin{bmatrix}
    a_{11}B & a_{12}B & \dots & a_{1n}B \\
    a_{21}B & a_{22}B & \dots & a_{2n}B \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}B & a{m2}B & \dots & a_{mn}B \end{bmatrix}
    \label{Kronecker} 
\end{equation}

\subsubsection{Khatri-Rao Product \(\odot\)}

The Khatri-Rao product of two matrices \(A\) and \(B\), each with the same number of columns, is a matrix composed of the Kronecker products of the columns in matrix \(A\) and the columns in matrix \(B\) with the same indices.

\begin{equation}
    A^{:\times n} \odot B^{:\times n} = \begin{bmatrix}
    a_{:,1} \otimes b_{:,1} & a_{:,2} \otimes b_{:,2} & \dots & a_{:,n} \otimes b_{:,n} \end{bmatrix}
    \label{Khatri-Rao}
\end{equation}

\subsubsection{Hadamard Product \(*\)}

The Hadamard product of two matrices \(A\) and \(B\) of the same dimensions is a matrix formed of the products of the elements in \(A\) and \(B\) with the same indices.

\begin{equation}
    A^{m \times n} * B^{m \times n} = \begin{bmatrix}
    a_{11}b_{11} & a_{12}b_{12} & \dots & a_{1n}b_{1n} \\
    a_{21}b_{21} & a_{22}b_{22} & \dots & a_{2n}b_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    a_{m1}b_{m1} & a_{m2}b_{m2} & \dots & a_{mn}b_{mn} \end{bmatrix} 
    \label{Hadamard}
\end{equation}

\subsection{Tensor Decompositions}

% add intro

\subsubsection{CP Decomposition}

The canonical polyadic, or CP Decomposition, decomposes a tensor into vectors.

\begin{equation}
    T = \sum_{r-1}^R u_r^{(1)} \circ  u_r^{(2)} \circ \dots  u_r^{(R)}\
    \label{CP}
\end{equation} % review notation

% add info re computation

\subsubsection{Tucker Decomposition}

The Tucker decomposition decomposes a tensor into a core tensor and factored matrices. % add more explanantion

\begin{equation}
    T = C\prod_{n=1}^N A^n
    \label{Tucker}
\end{equation} % review notation

where \(C\) is the core tensor.

% add info re computation

\subsubsection{Tensor Train}

The tensor train decomposition decomposes a tensor into a product of third-order tensors. It is used when a tensor is too large for the CP decomposition to be practical. 

% add representative equation, info on computation

\section{Selecting A Network Architecture}

\section{Convolutional Neural Networks}

% add intro

\subsection{What Is Convolution?}

Many young people who are active on social media use filters on their pictures before posting them. These filters apply different effects to the photo, such as blurring, making the image black and white, or adding cartoon-like effects. When you apply a filter to a photo, you are using convolution. Convolution is an operation that takes the aggregation of the element-wise product of a tensor of input data and a (typically smaller) tensor, called a kernel. This produces a feature map. In the context of image processing, the original image is the input data, the filter is the kernel, and the filtered image is the feature map. 

This is an example of discrete convolution, which is the case that we will be primarily concerned with. % In the interest of providing a complete resource, equations for discrete and continuous convolution in one dimension are provided below \cite{Goodfellow-et-al-2016}.

In one dimension, discrete convolution is denoted
\begin{equation}
    \underbrace{s(i)}_{\substack{\text{feature} \\ \text{map}}} = (x*y)(i) = \sum\limits_{\substack{m}} \underbrace{x(m)}_{\substack{\text{input} \\ \text{data}}}\underbrace{y(i-m)}_\text{kernel}
    \label{1D-discrete-convolution}\
\end{equation}


% In the continuous case, equation \ref{1D-discrete-convolution} becomes
% \begin{equation}
    % s(i) = (x*y)(i) = \int_m x(m)y(i-m)dm
    % \label{1D-continuous-convolution}\
% \end{equation}

One of the most common applications of CNNs is in digital image processing. In the case of a black and white image, the input data is a two-dimensional tensor. Discrete convolution in two dimensions can be performed as follows \cite{Goodfellow-et-al-2016}. %check this

\begin{equation}
    s(i,j) = (x*y)(i,j)\sum\limits_{\substack{m}}\sum\limits_{\substack{n}} x(i,j)y(i-m,j-n)
    \label{2D-discrete-convolution}
\end{equation}

Since convolution is commutative, equation \ref{flipped-2D-discrete-convolution} also holds. 

\begin{equation}
    s(i,j) = (y*x)(i,j)\sum\limits_{\substack{m}}\sum\limits_{\substack{n}} x(i-m,j-n)y(i,j)
    \label{flipped-2D-discrete-convolution}
\end{equation}

Two-dimensional discrete convolution is equivalent to reversing the row and column indices of the kernel, performing element-wise multiplication, and taking the sum of the products. In the context of deep learning, the term ``convolution" often refers to a similar operation called cross-correlation (equation). This is equivalent to convolution without the reversing of the matrix indices \cite{Goodfellow-et-al-2016}. 

\begin{equation}
    s(i,j) = (y*x)(i,j)\sum\limits_{\substack{m}}\sum\limits_{\substack{n}} x(i+m,j+n)y(i,j)
    \label{cross-correlation}
\end{equation}


% discrete case: \((a * b)_n = \sum\limits_{\substack{i,j \\ i+j=n}} a_i \cdot b_j\)

\subsection{Image Classification Example}

\section{Recurrent Neural Networks}

\section{Generative Models}

\subsection{Generative Adversarial Neural Networks}

\subsection{Variational Autoencoders}

\subsection{Quantization}

% can quantize weights or biases

% linear quantization ( see L4 quantization theory)

% example - 32-bit to 8-bit

% quanto library


\end{document}