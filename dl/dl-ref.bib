@book{Goodfellow-et-al-2016,
	title        = {Deep Learning},
	author       = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
	year         = 2016,
	publisher    = {MIT Press},
	url          = {http://www.deeplearningbook.org}
}
@book{hands-on-ml,
	title        = {Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, Third Edition},
	author       = {Aur{\'{e}}lien G{\'{e}}ron},
	year         = 2022,
	publisher    = {O'Reilly Media, Inc}
}
@misc{montufar2014,
	title        = {On the Number of Linear Regions of Deep Neural Networks},
	author       = {Guido Mont{\'{u}}far and Razvan Pascanu and Kyunghyun Cho and Yoshua Bengio},
	year         = 2014,
	eprint       = {1402.1869},
	archiveprefix = {arXiv},
	primaryclass = {id='stat.ML' full\_name='Machine Learning' is\_active=True alt\_name=None in\_archive='stat' is\_general=False description='Covers machine learning papers (supervised, unsupervised, semi-supervised learning, graphical models, reinforcement learning, bandits, high dimensional inference, etc.) with a statistical or theoretical grounding'}
}
@book{pml1book,
	title        = {Probabilistic Machine Learning: An Introduction},
	author       = {Kevin P. Murphy},
	year         = 2022,
	publisher    = {MIT Press},
	url          = {probml.ai}
}
@article{Srivastava14,
	title        = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
	author       = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
	year         = 2014,
	journal      = {Journal of Machine Learning Research},
	volume       = 15,
	number       = 56,
	pages        = {1929--1958},
	url          = {http://jmlr.org/papers/v15/srivastava14a.html}
}
@misc{tensorflow2015-whitepaper,
	title        = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
	author       = {Mart{\'i}n Abadi and Paul Barham and Jianmin Chen and Zhifeng Chen and Andy Davis and Jeffrey Dean and Matthieu Devin and Sanjay Ghemawat and Geoffrey Irving and Michael Isard and Manjunath Kudlur and Josh Levenberg and Rajat Monga and Sherry Moore and Derek G. Murray and Benoit Steiner and Paul Tucker and Vijay Vasudevan and Pete Warden and Martin Wicke and Yuan Yu and Xiaoqiang Zheng},
	year         = 2015,
	url          = {https://www.tensorflow.org/},
	note         = {Software available from tensorflow.org}
}

@inproceedings{sutskever13,
	title        = {On the importance of initialization and momentum in deep learning},
	author       = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
	year         = 2013,
	booktitle    = {Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28},
	location     = {Atlanta, GA, USA},
	publisher    = {JMLR.org},
	series       = {ICML'13},
	pages        = {III–1139–III–1147},
	abstract     = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.}
}

@misc{zeiler2012,
	title        = {ADADELTA: An Adaptive Learning Rate Method},
	author       = {Matthew D. Zeiler},
	year         = 2012,
	url          = {https://arxiv.org/abs/1212.5701},
	eprint       = {1212.5701},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}

@book{pml2Book,
	author = {Kevin P. Murphy},
 	title = {Probabilistic Machine Learning: Advanced Topics},
	publisher = {MIT Press},
	year = 2023,
	url = {http://probml.github.io/book2}
}

@misc{riebesell_tikz_2020,
	title = {Collection of standalone TikZ images},
	author = {Riebesell, Janosh and Bringuier, Stefan},
	date = {2020-08-09},
	year = {2020},
	doi = {10.5281/zenodo.7486911},
	url = {https://github.com/janosh/tikz},
	note = {
	MIT License

	Copyright (c) 2021 Janosh Riebesell

	Permission is hereby granted, free of charge, to any person obtaining a copy
	of this software and associated documentation files (the "Software"), to deal
	in the Software without restriction, including without limitation the rights
	to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
	copies of the Software, and to permit persons to whom the Software is
	furnished to do so, subject to the following conditions:

	The above copyright notice and this permission notice shall be included in all
	copies or substantial portions of the Software.

	The software is provided "as is", without warranty of any kind, express or
	implied, including but not limited to the warranties of merchantability,
	fitness for a particular purpose and noninfringement. In no event shall the
	authors or copyright holders be liable for any claim, damages or other
	liability, whether in an action of contract, tort or otherwise, arising from,
	out of or in connection with the software or the use or other dealings in the
	software.
	},
	version = {0.1.0},
}

@misc{neutelings_tikz_2022,
    author   = {Izaak Neutelings},
    title    = {Neural networks},
    year     = {2022},
    url      = {https://tikz.net/neural_networks/#full_code},
    note     = {Licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit\url{http://creativecommons.org/licenses/by/4.0/}. Changes were made to this work.}
}   