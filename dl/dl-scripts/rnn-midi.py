# -*- coding: utf-8 -*-
"""rnn-midi.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9tBYClAwnIudqN_W56iwoaUwL0gfPLf

# LSTM for Music Generation with Pytorch Lightning

This notebook provides a guide for training an RNN for music generation. It was inspired by [this Tensorflow tutorial](https://www.tensorflow.org/tutorials/audio/music_generation), but uses Pytorch and Lightning. Training / validation data consists of 75 classical piano examples from the [ADL Piano Midi Dataset](https://github.com/lucasnfe/adl-piano-midi/blob/master/README.md).
"""

! pip install pretty_midi lightning

from google.colab import drive
drive.mount('/content/drive')

import os
import glob
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pretty_midi
from torch.utils.data import Dataset, DataLoader, random_split
from lightning.pytorch.callbacks.early_stopping import EarlyStopping
import lightning as L
import torch.nn.functional as F

L.seed_everything(seed=42)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

class MidiDataset(Dataset):
    def __init__(self, midi_dir: str, seq_length: int, quantize_step=0.125, quantize_duration=0.125, vocab_size=128):
        self.midi_dir = midi_dir
        self.seq_length = seq_length
        self.quantize_step = quantize_step
        self.quantize_duration = quantize_duration
        self.vocab_size = vocab_size
        self.data = self.load_and_process_data()

    def load_and_process_data(self):
        midi_files = glob.glob(os.path.join(self.midi_dir, '*.mid'))
        data = []
        for midi_file in midi_files:
            merged_midi_data = self.merge_tracks(midi_file)
            notes = sorted(merged_midi_data.instruments[0].notes, key=lambda note: note.start)
            processed_notes = self.process_notes(notes)
            for i in range(len(processed_notes) - self.seq_length):
                data.append(processed_notes[i:i + self.seq_length + 1])
        return data

    def merge_tracks(self, file_path):
        midi_data = pretty_midi.PrettyMIDI(file_path)
        merged_midi = pretty_midi.PrettyMIDI()
        merged_instrument = pretty_midi.Instrument(program=0)

        all_notes = []
        for instrument in midi_data.instruments:
            all_notes.extend(instrument.notes)

        sorted_notes = sorted(all_notes, key=lambda note: note.start)
        merged_instrument.notes.extend(sorted_notes)
        merged_midi.instruments.append(merged_instrument)

        return merged_midi

    def process_notes(self, notes):
        processed = []
        for i in range(1, len(notes)):
            pitch = notes[i].pitch
            step = round((notes[i].start - notes[i-1].start) / self.quantize_step) * self.quantize_step
            duration = round((notes[i].end - notes[i].start) / self.quantize_duration) * self.quantize_duration
            processed.append([pitch, step, duration])
        return processed

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sequence = self.data[idx]
        inputs = torch.tensor(sequence[:-1], dtype=torch.float32)
        pitch_target = torch.tensor(sequence[-1][0], dtype=torch.long)
        step_target = torch.tensor(sequence[-1][1], dtype=torch.float32)
        duration_target = torch.tensor(sequence[-1][2], dtype=torch.float32)
        return inputs, pitch_target, step_target, duration_target

class MidiDataModule(L.LightningDataModule):
    def __init__(self, midi_dir: str, seq_length: int, batch_size: int, val_split=0.2, quantize_step=0.125, quantize_duration=0.125):
        super().__init__()
        self.midi_dir = midi_dir
        self.seq_length = seq_length
        self.batch_size = batch_size
        self.val_split = val_split
        self.quantize_step = quantize_step
        self.quantize_duration = quantize_duration

    def setup(self, stage=None):
        full_dataset = MidiDataset(self.midi_dir, self.seq_length, self.quantize_step, self.quantize_duration)

        val_size = int(len(full_dataset) * self.val_split)
        train_size = len(full_dataset) - val_size
        self.train_dataset, self.val_dataset = random_split(full_dataset, [train_size, val_size])

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size)

class MIDIModel(L.LightningModule):
    def __init__(self, input_size, hidden_size, output_size, dropout_rate=0.1, weight_decay=1e-5):
        super(MIDIModel, self).__init__()
        self.rnn = nn.LSTM(input_size, hidden_size, batch_first=True) # LSTM layer

        self.dropout = nn.Dropout(dropout_rate) # dropout layer

        self.fc_pitch = nn.Linear(hidden_size, output_size) # pitch output
        self.fc_step = nn.Linear(hidden_size, 1) # step output
        self.fc_duration = nn.Linear(hidden_size, 1) # duration output

        self.loss_fn_pitch = nn.CrossEntropyLoss() # pitch loss
        self.loss_fn_step_duration = nn.MSELoss() # step and duration loss

        self.weight_decay = weight_decay

    def forward(self, x):
        rnn_out, _ = self.rnn(x)
        rnn_out = self.dropout(rnn_out)
        pitch_out = self.fc_pitch(rnn_out[:, -1, :])
        step_out = self.fc_step(rnn_out[:, -1, :])
        duration_out = self.fc_duration(rnn_out[:, -1, :])

        return {'pitch': pitch_out, 'step': step_out, 'duration': duration_out}

    def training_step(self, batch, batch_idx):
        inputs, pitch_target, step_target, duration_target = batch
        predictions = self(inputs)

        loss_pitch = self.loss_fn_pitch(predictions['pitch'], pitch_target)
        loss_step = self.loss_fn_step_duration(predictions['step'], step_target.unsqueeze(-1))
        loss_duration = self.loss_fn_step_duration(predictions['duration'], duration_target.unsqueeze(-1))

        loss = loss_pitch + loss_step + loss_duration
        self.log('train_loss', loss, prog_bar=True, logger=True) # log the training loss

        return loss

    def validation_step(self, batch, batch_idx):
        inputs, pitch_target, step_target, duration_target = batch
        predictions = self(inputs)

        loss_pitch = self.loss_fn_pitch(predictions['pitch'], pitch_target)
        loss_step = self.loss_fn_step_duration(predictions['step'], step_target.unsqueeze(-1))
        loss_duration = self.loss_fn_step_duration(predictions['duration'], duration_target.unsqueeze(-1))

        loss = loss_pitch + loss_step + loss_duration
        self.log('val_loss', loss, prog_bar=True, logger=True) # log the validation loss

        return loss

    def configure_optimizers(self):
        optimizer = optim.Adam(self.parameters(), lr=0.001, weight_decay=self.weight_decay)

        return optimizer

midi_dir = "/my-midi-directory" # replace with the path to the folder containing your MIDI files
seq_length = 25
batch_size = 64
vocab_size = 128
input_size = 3
hidden_size = 128
output_size = vocab_size

data_module = MidiDataModule(midi_dir=midi_dir, seq_length=seq_length, batch_size=batch_size)

model = MIDIModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size)

early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=0.00, patience=3, verbose=False, mode="min")
trainer = L.Trainer(max_epochs=20, callbacks=[early_stop_callback], accelerator="gpu" if torch.cuda.is_available() else "cpu", devices=1)

trainer.fit(model, data_module)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir=lightning_logs/

def get_seed_sequence(midi_file_path, device, num_notes=25, skip_notes=25, quantize_step=0.125, quantize_duration=0.125):
    midi_data = pretty_midi.PrettyMIDI(midi_file_path)

    merged_notes = []
    for instrument in midi_data.instruments:
        merged_notes.extend(instrument.notes)

    notes = sorted(merged_notes, key=lambda note: note.start)

    if len(notes) < (skip_notes + num_notes):
        print(f"Not enough notes after skipping the first {skip_notes}. Using all available notes after skipping.")
        start_index = skip_notes if skip_notes < len(notes) else 0
        num_notes = len(notes) - start_index
    else:
        start_index = skip_notes

    seed_sequence = []
    previous_start = notes[start_index].start if notes else 0.0

    for note in notes[start_index:start_index + num_notes]:
        pitch = note.pitch

        step = note.start - previous_start
        duration = note.end - note.start

        step = round(step / quantize_step) * quantize_step
        duration = round(duration / quantize_duration) * quantize_duration

        seed_sequence.append([pitch, step, duration])
        previous_start = note.start

    seed_sequence_tensor = torch.tensor(seed_sequence, dtype=torch.float32).to(device)
    return seed_sequence_tensor

midi_file_path = "path-to/seed-file.mid" # make sure this isn't part of the training / validation dataset
seed_sequence = get_seed_sequence(midi_file_path, device=device, num_notes=25, quantize_step=0.125, quantize_duration=0.125)

# print(seed_sequence)

def generate_notes(model, device, seed_sequence, sequence_length, vocab_size=128, temperature=1.0, quantize_step=0.125, quantize_duration=0.125, max_silence=1.0):
    model.eval()  # set model to evaluation mode
    generated_sequence = []

    current_sequence = seed_sequence  # Initialize with the seed sequence

    for _ in range(sequence_length):
        current_sequence_tensor = current_sequence.unsqueeze(0).to(device)

        with torch.no_grad():
            predictions = model(current_sequence_tensor)

        pitch_logits = predictions['pitch'].squeeze() / temperature # apply temperature scaling

        pitch_probs = F.softmax(pitch_logits, dim=-1)

        predicted_pitch = torch.multinomial(pitch_probs, num_samples=1).item() # sample pitch from multinomial distribution

        predicted_step = predictions['step'].squeeze().item()
        predicted_duration = predictions['duration'].squeeze().item()

        predicted_step = min(predicted_step, max_silence) # max silence

        predicted_step = round(predicted_step / quantize_step) * quantize_step

        predicted_duration = round(predicted_duration / quantize_duration) * quantize_duration

        generated_sequence.append([predicted_pitch, predicted_step, predicted_duration])

        new_note = torch.tensor([[predicted_pitch, predicted_step, predicted_duration]], dtype=torch.float32).to(device)
        current_sequence = torch.cat((current_sequence[1:], new_note)) 

    return generated_sequence

model.to(device)
generated_sequence = generate_notes(model, device=device, seed_sequence=seed_sequence, sequence_length=400, temperature=1)


def notes_to_midi(generated_sequence, output_path="generated_music.mid"):
    midi = pretty_midi.PrettyMIDI()
    instrument = pretty_midi.Instrument(program=0)

    start_time = 0

    for note in generated_sequence:
        pitch = int(note[0])
        step = note[1]
        duration = note[2]

        start_time += step
        end_time = start_time + duration

        midi_note = pretty_midi.Note(
            velocity=100,
            pitch=pitch,
            start=start_time,
            end=end_time
        )
        instrument.notes.append(midi_note)

    midi.instruments.append(instrument)
    midi.write(output_path)

notes_to_midi(generated_sequence, output_path="generated_music.mid")